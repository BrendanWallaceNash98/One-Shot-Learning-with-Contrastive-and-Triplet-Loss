{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23308f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import difflib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import *\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d0455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 96\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "'''\n",
    "the current dataset and epochs are not representive of what was used for the report but more so what was needed \n",
    "to run the code in spyder to convert from jupyter notebook to py.file. Accuracy will be lower on triplet loss CNN\n",
    "as epochs and dataset were reduced. Please refer to the the jupytern notebooks attached in the submission to see the \n",
    "actual parameters used.\n",
    "'''\n",
    "\n",
    "def my_team():\n",
    "    '''\n",
    "    Return the list of the team members of this assignment submission as a list\n",
    "    of triplet of the form (student_number, first_name, last_name)\n",
    "    \n",
    "    '''\n",
    "    return [(9993304, \"Brendan\", \"Wallace-Nash\"),(9300449, \"Min-Pu\", \"Tsai\"), (10661450, \"Bingqing\", \"Qian\")]\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def triplet_loss_test():\n",
    "    '''\n",
    "    Read back the arrays x_train, y_train, x_test and y_test\n",
    "    from the npz file named 'mnist_dataset.npz'.\n",
    "    Then, print the shape and dtype of these numpy arrays.\n",
    "    \n",
    "    '''\n",
    "    def triplet_loss(y_true, y_pred, margin = 0.4):\n",
    "        \"\"\"\n",
    "        Implementation of the triplet loss function\n",
    "        Arguments:\n",
    "        y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "        y_pred -- python list containing three objects:\n",
    "                anchor -- the encodings for the anchor data\n",
    "                positive -- the encodings for the positive data (similar to anchor)\n",
    "                negative -- the encodings for the negative data (different from anchor)\n",
    "        Returns:\n",
    "        loss -- real number, value of the loss\n",
    "        \"\"\"\n",
    "    \n",
    "        anchor = y_pred[0]\n",
    "        positive = y_pred[1]\n",
    "        negative = y_pred[2]\n",
    "    \n",
    "        # distance between the anchor and the positive\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor-positive),axis=1)\n",
    "    \n",
    "        # distance between the anchor and the negative\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor-negative),axis=1)\n",
    "    \n",
    "        # compute loss\n",
    "        basic_loss = pos_dist-neg_dist+margin\n",
    "        loss = tf.maximum(basic_loss,0.0)\n",
    "        loss = tf.reduce_mean(loss)     \n",
    "        return loss\n",
    "    \n",
    "    #Test implementation of triplet loss function \n",
    "    num_data = 10\n",
    "    feat_dim = 6\n",
    "    margin = 0.2\n",
    "    \n",
    "    embeddings = [np.random.rand(num_data, feat_dim).astype(np.float32),\n",
    "                  np.random.rand(num_data, feat_dim).astype(np.float32),\n",
    "                  np.random.rand(num_data, feat_dim).astype(np.float32)]\n",
    "    labels = np.random.randint(0, 1, size=(num_data)).astype(np.float32)\n",
    "      \n",
    "    #Compute loss with numpy\n",
    "    loss_np = 0.\n",
    "    anchor = embeddings[0]\n",
    "    positive = embeddings[1]\n",
    "    negative = embeddings[2]\n",
    "    for i in range(num_data):\n",
    "        pos_dist = np.sum(np.square(anchor[i] - positive[i]))\n",
    "        neg_dist = np.sum(np.square(anchor[i] - negative[i]))\n",
    "        loss_np += max(pos_dist-neg_dist+margin, 0.)\n",
    "    loss_np /= num_data\n",
    "    print('Triplet loss computed with numpy', loss_np)\n",
    "    \n",
    "    # Compute the loss in TF\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    loss_tf = triplet_loss(labels, embeddings, margin)\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        loss_tf_val = sess.run(loss_tf)\n",
    "        print('Triplet loss computed with tensorflow', loss_tf_val)\n",
    "    assert np.allclose(loss_np, loss_tf_val)\n",
    "\n",
    "\n",
    "\n",
    "def data_prepare():\n",
    "    '''\n",
    "    Prepare the dataset for training and testing from tensorflow dataset.\n",
    "    \n",
    "    Arguments:\n",
    "        (None)\n",
    "        \n",
    "    Returns:\n",
    "        arrayImage -- numpy array images of alphabets\n",
    "        arrayAlph -- names of alphabets \n",
    "        x_test --  numpy array images of alphabets in test data\n",
    "        y_test -- names of alphabets in test data\n",
    "        x_train -- images of alphabets in train data. empty variable at the moment.\n",
    "        y_train -- names of alphabets in train data. empty variable at the moment.\n",
    "    '''\n",
    "    #Load omniglot dataset from tensorflow dataset, divide it into train data and test data by 80%.\n",
    "    ds, ds_info = tfds.load('omniglot', split=['train[:80%]','test'],with_info=True, as_supervised = False)\n",
    "    train = tfds.as_dataframe(ds[0])\n",
    "    test = tfds.as_dataframe(ds[1])\n",
    "    \n",
    "    #we set this valus just to run quicker.\n",
    "    train = train[:16000]\n",
    "    test = test[:4000]\n",
    "    \n",
    "    #Create sets from train data by the attributes, name and image of alphbet in dataset;\n",
    "    arrayAlph = train[\"alphabet\"].to_list()\n",
    "    arrayImage = train[\"image\"].to_list()\n",
    "    #do the same thing to test data.\n",
    "    y_test = test[\"alphabet\"].to_list()\n",
    "    x_test = test[\"image\"].to_list()\n",
    "    \n",
    "    #Empty at the moment\n",
    "    x_train = [] \n",
    "    y_train = []\n",
    "    \n",
    "    #Convert images to numpy array\n",
    "    x_test = np.array(x_test)\n",
    "    arrayImage = np.array(arrayImage)\n",
    "    \n",
    "    return arrayImage, arrayAlph, x_test, y_test, x_train, y_train\n",
    "\n",
    "\n",
    "def global_variable1(arrayImage, arrayAlph):  \n",
    "    '''\n",
    "    Training data. Prepare the variables will be used next.\n",
    "    \n",
    "    Arguments:\n",
    "        arrayImage, arrayAlph\n",
    "        \n",
    "    Returns:\n",
    "        label -- training label list. 'Label' here means Positive/Negtive \n",
    "                 results of corresponding triplet. Recording as 1/0               \n",
    "        tripletLitst -- training ist of triplet. A triplet contains Anchor image, \n",
    "                        Positive image, and Negtive image\n",
    "    '''\n",
    "    label = []\n",
    "    tripletLitst = []\n",
    "    imageL1 = arrayImage\n",
    "    alphL1 = arrayAlph\n",
    "    imageL2 = arrayImage.copy()\n",
    "    alphL2 = arrayAlph.copy()\n",
    "    imageL3 = arrayImage.copy()\n",
    "    alphL3 = arrayAlph.copy()\n",
    "    \n",
    "    shuffleList1 = list(zip(imageL2, alphL2))\n",
    "    random.shuffle(shuffleList1)\n",
    "    imageL2, alphL2 = zip(*shuffleList1)\n",
    "    \n",
    "    shuffleList2 = list(zip(imageL3, alphL3))\n",
    "    random.shuffle(shuffleList2)\n",
    "    imageL3, alphL3 = zip(*shuffleList2)\n",
    "    \n",
    "    for i in range(len(imageL1)):\n",
    "        a = []\n",
    "        p = []\n",
    "        if alphL1[i] == alphL2[i]:\n",
    "            a.append(imageL1[i])\n",
    "            p.append(imageL2[i])\n",
    "            for x in range(len(imageL1)):\n",
    "                n = []\n",
    "                if alphL1[i] != alphL3[x]:\n",
    "                    n.append(imageL3[x])\n",
    "                    label.append(0)\n",
    "                    tripletLitst.append([a, p, n])\n",
    "    \n",
    "    tripletLitst = tripletLitst[:20000]\n",
    "    label = label[:20000]\n",
    "    \n",
    "    \n",
    "    \n",
    "    label = np.array(label)\n",
    "    \n",
    "    tripletLitst = np.array(tripletLitst)\n",
    "    \n",
    "    return label, tripletLitst\n",
    "    \n",
    "    \n",
    "def global_variable2(arrayImage, arrayAlph):  \n",
    "    '''\n",
    "    Testing data. Prepare the variables will be used next.\n",
    "    \n",
    "    Arguments:\n",
    "        arrayImage, arrayAlph\n",
    "        \n",
    "    Returns:\n",
    "        testlabel -- testing label list. 'Label' here means Positive/Negtive \n",
    "                     results of corresponding triplet. Recording as 1/0 \n",
    "        testList -- testing ist of triplet. A triplet contains Anchor image, \n",
    "                    Positive image, and Negtive image\n",
    "    '''\n",
    "    testlabel = []\n",
    "    testList = []\n",
    "    imageL1 = arrayImage\n",
    "    alphL1 = arrayAlph\n",
    "    imageL2 = arrayImage.copy()\n",
    "    alphL2 = arrayAlph.copy()\n",
    "    imageL3 = arrayImage.copy()\n",
    "    alphL3 = arrayAlph.copy()\n",
    "    \n",
    "    shuffleList1 = list(zip(imageL2, alphL2))\n",
    "    random.shuffle(shuffleList1)\n",
    "    imageL2, alphL2 = zip(*shuffleList1)\n",
    "    \n",
    "    shuffleList2 = list(zip(imageL3, alphL3))\n",
    "    random.shuffle(shuffleList2)\n",
    "    imageL3, alphL3 = zip(*shuffleList2)\n",
    "    \n",
    "    for i in range(len(imageL1)):\n",
    "        a = []\n",
    "        p = []\n",
    "        if alphL1[i] == alphL2[i]:\n",
    "            a.append(imageL1[i])\n",
    "            p.append(imageL2[i])\n",
    "            for x in range(len(imageL1)):\n",
    "                n = []\n",
    "                if alphL1[i] != alphL3[x]:\n",
    "                    n.append(imageL3[x])\n",
    "                    testlabel.append(0)\n",
    "                    testList.append([a, p, n])\n",
    "                    \n",
    "                elif alphL1[i] == alphL3[x]:\n",
    "                    n.append(imageL3[x])\n",
    "                    testlabel.append(1)\n",
    "                    testList.append([a, p, n])\n",
    "\n",
    "    valuesTest, countsTest = np.unique(testlabel, return_counts=True)\n",
    "\n",
    "    \n",
    "    testList = testList[:4000]\n",
    "    testlabel = testlabel[:4000]\n",
    "    testList = np.array(testList)\n",
    "    testlabel = np.array(testlabel)\n",
    "    \n",
    "    return testlabel, testList\n",
    " \n",
    "   \n",
    "\n",
    "def dataProcess(arrayImage, arrayAlph, x_test, y_test, tripletLitst):    \n",
    "    '''\n",
    "    Processing data to numpy array. Including training data nad testing data\n",
    "    \n",
    "    Argument:\n",
    "        arrayImage -- numpy array images of alphabets\n",
    "        arrayAlph -- names of alphabets \n",
    "        x_test --  numpy array images of alphabets in test data\n",
    "        y_test -- names of alphabets in test data\n",
    "        x_train -- images of alphabets in train data\n",
    "        y_train -- names of alphabets in train data\n",
    "        tripletLitst -- training ist of triplet. A triplet contains Anchor image, \n",
    "                        Positive image, and Negtive image\n",
    "    Return:\n",
    "        same variables but convert to numpy array, reshape and/or reduce to \n",
    "        resonable sample quantity\n",
    "    '''\n",
    "    lenX = round(len(tripletLitst)*0.20)\n",
    "    lenY = round(len(label)*0.20)\n",
    "    x_train = tripletLitst[lenX:]\n",
    "    x_test = tripletLitst[lenX:]\n",
    "    y_train = tripletLitst[:lenX]\n",
    "    y_test = tripletLitst[:lenX]\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    x_train = x_train.reshape(len(x_train), 3, 105, 105, 3)\n",
    "    y_train = y_train.reshape(len(y_train), 3, 105, 105, 3)\n",
    "    \n",
    "    arrayAlph = np.array(arrayAlph)\n",
    "    arrayImage = np.array(arrayImage)\n",
    "    y_test = y_test[:16000]    \n",
    "    x_test = x_test[:16000]\n",
    "    \n",
    "    return arrayImage, arrayAlph, x_test, y_test, x_train, y_train\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Triple loss function\n",
    "    \n",
    "    Argument:\n",
    "        y_true -- exact valuse of y\n",
    "        y_pred -- prediction values of y\n",
    "    Return:\n",
    "        K.mean\n",
    "    '''\n",
    "    anchor_out = y_pred[:, 0:100]\n",
    "    positive_out = y_pred[:, 100:200]\n",
    "    negative_out = y_pred[:, 200:300]\n",
    "    \n",
    "    pos_dist = K.sum(K.abs(anchor_out - positive_out), axis=1)\n",
    "    neg_dist = K.sum(K.abs(anchor_out - negative_out), axis=1)\n",
    "    \n",
    "    probs = K.softmax([pos_dist, neg_dist], axis=0)\n",
    "    \n",
    "    return K.mean(K.abs(probs[0]) + K.abs(1.0 - probs[1]))\n",
    "\n",
    "def contrastive_loss_patch():\n",
    "        '''\n",
    "    Embedding and builds the contrastive model. Prints epochs.\n",
    "    \n",
    "    Arguments:\n",
    "         (None)\n",
    "    Returns:\n",
    "        siamese -- Model for contrative loss\n",
    "    '''\n",
    "        ds, ds_info = tfds.load('omniglot', split=['train','test'],with_info=True, as_supervised = False)\n",
    "        trainDF = tfds.as_dataframe(ds[0], ds_info)\n",
    "        testDF = tfds.as_dataframe(ds[1], ds_info)\n",
    "        \n",
    "        trainDF = trainDF[:16000]\n",
    "        testDF = testDF[:4000]\n",
    "        \n",
    "        def make_positive_pairs(df):\n",
    "            list_image = []    \n",
    "            pairList= []\n",
    "            \n",
    "            uniqueAlph = df['alphabet'].unique()\n",
    "            \n",
    "            for i in range(len(uniqueAlph)):\n",
    "                alphDF = df[df['alphabet'] == i]\n",
    "                charUnique = alphDF['alphabet_char_id'].unique()\n",
    "                for i in range(len(charUnique)):\n",
    "                    dfImage = alphDF[alphDF['alphabet_char_id'] == charUnique[i]]\n",
    "                    imagedf = dfImage['image'].copy()\n",
    "                    imageList = imagedf.values.tolist()             \n",
    "                    list_image.append(imageList[0])      \n",
    "                x = 0 \n",
    "                pair_list = []\n",
    "                pair_target = []\n",
    "                list_image2 = list_image.copy()\n",
    "                for i in range(len(list_image)):\n",
    "                    for x in range(len(list_image)):\n",
    "                        pair_list.append([list_image[i], list_image2[x]])\n",
    "                        pair_target.append(1.0)                 \n",
    "                random.shuffle(pair_list)                  \n",
    "            return pair_list, pair_target\n",
    "        \n",
    "        def get_pairs(data):\n",
    "            df = data.sort_values(by=['alphabet'])\n",
    "            \n",
    "            df_pairs_1 = df[df.index % 2 != 0]\n",
    "            \n",
    "            df_pairs_2 = df[df.index % 2 != 1]\n",
    "            \n",
    "            df_pairs_1 = shuffle(df_pairs_1)\n",
    "            \n",
    "            imagedf = df_pairs_1['image'].copy()\n",
    "            imageList = imagedf.values.tolist()\n",
    "            \n",
    "            alphabetdf = df_pairs_1['alphabet'].copy()\n",
    "            alphabetList = alphabetdf.values.tolist()\n",
    "            \n",
    "            imagedf2 = df_pairs_2['image'].copy()\n",
    "            imageList2 = imagedf.values.tolist()\n",
    "            \n",
    "            alphabetdf2 = df_pairs_2['alphabet'].copy()\n",
    "            alphabetList2 = alphabetdf.values.tolist()\n",
    "            \n",
    "            imageList2 = shuffle(imageList2)\n",
    "            imageList = shuffle(imageList)\n",
    "            alphabetList = shuffle(alphabetList)\n",
    "            alphabetList2 = shuffle(alphabetList2)\n",
    "            \n",
    "            trainList = []\n",
    "            testList = []\n",
    "            \n",
    "            for i in range(len(alphabetList)):\n",
    "                loopList = []\n",
    "                pos = 1.0\n",
    "                neg = 0.0\n",
    "                if alphabetList[i] == alphabetList2[i]:\n",
    "                    loopList.append(imageList[i].astype(float))\n",
    "                    loopList.append(imageList2[i].astype(float))\n",
    "                    testList.append(pos)\n",
    "                    trainList.append(loopList)\n",
    "                else:\n",
    "                    loopList.append(imageList[i].astype(float))\n",
    "                    loopList.append(imageList2[i].astype(float))\n",
    "                    testList.append(neg)\n",
    "                    trainList.append(loopList)\n",
    "            \n",
    "            return trainList, testList\n",
    "        \n",
    "        def count_neg_pairs(list_bin):\n",
    "            x = 1\n",
    "            count = []\n",
    "            for i in range(len(list_bin)):\n",
    "                if list_bin[i] == 0.0:\n",
    "                    count.append(x)\n",
    "                else:\n",
    "                    pass\n",
    "            print(len(count))\n",
    "        def combine_pairs(train1, test1, train2, test2):\n",
    "            for i in range(len(train2)):\n",
    "                train1.append(train2[i])\n",
    "            \n",
    "            \n",
    "            for i in range(len(test2)):\n",
    "                test1.append(test2[i])\n",
    "            \n",
    "            shuffleList = list(zip(train1, test1))\n",
    "                               \n",
    "            random.shuffle(shuffleList)\n",
    "            \n",
    "            train1, test1 = zip(*shuffleList)\n",
    "                               \n",
    "            train1 = np.array(train1)\n",
    "            test1 = np.array(test1)\n",
    "        \n",
    "            \n",
    "            return train1, test1\n",
    "        \n",
    "        trainX, trainY = get_pairs(trainDF)\n",
    "        trainXPos, trainYPos = make_positive_pairs(trainDF)\n",
    "        trainYPos = trainYPos[0:20000]\n",
    "        trainXFinal, trainYFinal= combine_pairs(trainXPos, trainYPos, trainX, trainY)\n",
    "        testX, testY = get_pairs(testDF)\n",
    "        testXPos, testYPos = make_positive_pairs(testDF)\n",
    "        testXPos = testXPos[:5000]\n",
    "        testYPos = testYPos[:5000]\n",
    "        testXFinal, testYFinal = combine_pairs(testXPos, testYPos, testX, testY)\n",
    "        lenX = round(len(trainXFinal)*0.20)\n",
    "        lenY = round(len(trainYFinal)*0.20)\n",
    "        X_train = trainXFinal[lenX:]\n",
    "        X_test = trainYFinal[lenX:]\n",
    "        y_train = trainXFinal[:lenY]\n",
    "        y_test = trainYFinal[:lenY]\n",
    "        X_train = X_train/255\n",
    "        y_train = y_train/255\n",
    "        batch_size = 96\n",
    "    \n",
    "        margin = 1  # Margin for constrastive loss.\n",
    "        # Provided two tensors t1 and t2\n",
    "        # Euclidean distance = sqrt(sum(square(t1-t2)))\n",
    "        def euclidean_distance(vects):\n",
    "            \"\"\"Find the Euclidean distance between two vectors.\n",
    "        \n",
    "            Arguments:\n",
    "                vects: List containing two tensors of same length.\n",
    "        \n",
    "            Returns:\n",
    "                Tensor containing euclidean distance\n",
    "                (as floating point value) between vectors.\n",
    "            \"\"\"\n",
    "        \n",
    "            x, y = vects\n",
    "            sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "            return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "        \n",
    "        \n",
    "        input = layers.Input((105, 105, 3))\n",
    "        x = tf.keras.layers.BatchNormalization()(input)\n",
    "        x = layers.Conv2D(4, (5, 5), activation=\"tanh\")(x)\n",
    "        x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "        x = layers.Conv2D(16, (5, 5), activation=\"tanh\")(x)\n",
    "        x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = layers.Dense(10, activation=\"tanh\")(x)\n",
    "        embedding_network = keras.Model(input, x)\n",
    "        \n",
    "        \n",
    "        input_1 = layers.Input((105, 105, 3))\n",
    "        input_2 = layers.Input((105, 105, 3))\n",
    "        \n",
    "        # As mentioned above, Siamese Network share weights between\n",
    "        # tower networks (sister networks). To allow this, we will use\n",
    "        # same embedding network for both tower networks.\n",
    "        tower_1 = embedding_network(input_1)\n",
    "        tower_2 = embedding_network(input_2)\n",
    "        \n",
    "        merge_layer = layers.Lambda(euclidean_distance)([tower_1, tower_2])\n",
    "        normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
    "        output_layer = layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "        siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "        \n",
    "        def loss(margin=1):\n",
    "            \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n",
    "        \n",
    "          Arguments:\n",
    "              margin: Integer, defines the baseline for distance for which pairs\n",
    "                      should be classified as dissimilar. - (default is 1).\n",
    "        \n",
    "          Returns:\n",
    "              'constrastive_loss' function with data ('margin') attached.\n",
    "          \"\"\"\n",
    "        \n",
    "            # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "            #                         true_value * square( max(margin-prediction, 0) ))\n",
    "            def contrastive_loss(y_true, y_pred):\n",
    "                \"\"\"Calculates the constrastive loss.\n",
    "        \n",
    "              Arguments:\n",
    "                  y_true: List of labels, each label is of type float32.\n",
    "                  y_pred: List of predictions of same length as of y_true,\n",
    "                          each label is of type float32.\n",
    "        \n",
    "              Returns:\n",
    "                  A tensor containing constrastive loss as floating point value.\n",
    "              \"\"\"\n",
    "        \n",
    "                square_pred = tf.math.square(y_pred)\n",
    "                margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "                return tf.math.reduce_mean(\n",
    "                    (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "                )\n",
    "        \n",
    "            return contrastive_loss\n",
    "        \n",
    "        siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n",
    "        history = siamese.fit(\n",
    "                [X_train[:,0], X_train[:,1]],\n",
    "                X_test[:],\n",
    "                validation_data=([y_train[:,0], y_train[:,1]], y_test[:]),\n",
    "                batch_size=96,\n",
    "                epochs=epochs,\n",
    "            callbacks=[callback])\n",
    "        \n",
    "        def get_result(X, Y):\n",
    "            predictions = []\n",
    "            counts = 0\n",
    "            for i in range(len(X)):\n",
    "                prediction = siamese.predict([X[i][0].reshape(-1, 105, 105, 3), X[i][1].reshape(-1, 105, 105, 3)])\n",
    "\n",
    "                if prediction > 0.5:\n",
    "                    predictions.append(1)\n",
    "                else:\n",
    "                    predictions.append(0)\n",
    "            for i in range(len(X)):\n",
    "                if predictions[i] == Y[i]:\n",
    "                    counts = counts+1\n",
    "            accurracy = (counts/len(X))*100\n",
    "                    \n",
    "            return accurracy \n",
    "        \n",
    "        test_acc = get_result(testXFinal, testYFinal)\n",
    "        train_acc = get_result(trainXFinal, trainYFinal)\n",
    "        #mixX = np.concatenate((trainXFinal[:1000],testXFinal[:1000]))\n",
    "        #mixY = np.concatenate((testYFinal[:1000],trainYFinal[:1000]))\n",
    "        \n",
    "        #mix_acc = get_result(mixX, mixY)\n",
    "        print(\"train accuracy is {}%\".format(train_acc))\n",
    "        print(\"test accuracy is {}%\".format(test_acc))\n",
    "        #print(\"mix of train and test accuracy is {}%\".format(mix_acc))\n",
    "    \n",
    "        return siamese\n",
    "        \n",
    "    \n",
    "\n",
    "def triplet_embedding(tripletLitst, label,testList, x_test, y_test, x_train, y_train):\n",
    "    '''\n",
    "    Embedding and builds the triplet model. Prints epochs.\n",
    "    \n",
    "    Arguments:\n",
    "        tripletLitst, label,testList, x_test, y_test, x_train, y_train\n",
    "    Returns:\n",
    "        triplet_model -- Model for triplet loss\n",
    "    '''\n",
    "    tripletLitst = np.array(tripletLitst)\n",
    "    tripletLitst = tripletLitst.reshape(len(tripletLitst), 3, 105, 105, 3)\n",
    "    testList = testList.reshape(len(testList), 3, 105, 105, 3)\n",
    "    \n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    label = np.array(label)\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    \n",
    "    input_layer = Input((105, 105, 3))\n",
    "    x = Conv2D(32, 3, activation='relu')(input_layer)\n",
    "    x = Conv2D(32, 3, activation='relu')(x)\n",
    "    x = MaxPool2D(2)(x)\n",
    "    x = Conv2D(64, 3, activation='relu')(x)\n",
    "    x = Conv2D(64, 3, activation='relu')(x)\n",
    "    x = MaxPool2D(2)(x)\n",
    "    x = Conv2D(128, 3, activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    model = Model(input_layer, x)\n",
    "    \n",
    "    triplet_model_a = Input((105, 105, 3))\n",
    "    triplet_model_p = Input((105, 105, 3))\n",
    "    triplet_model_n = Input((105, 105, 3))\n",
    "    triplet_model_out = Concatenate()([model(triplet_model_a), model(triplet_model_p), model(triplet_model_n)])\n",
    "    triplet_model = Model([triplet_model_a, triplet_model_p, triplet_model_n], triplet_model_out)\n",
    "    \n",
    "    triplet_model.compile(loss=triplet_loss, optimizer='adam')\n",
    "    history = triplet_model.fit([x_train[:,0],\n",
    "                   x_train[:,1], x_train[:,2]], \n",
    "                  x_test[:], validation_data=([y_train[:,0],\n",
    "                   y_train[:,1], y_train[:,2]], \n",
    "                  y_test[:]), batch_size = 96, epochs=epochs, \n",
    "                  callbacks=[callback])\n",
    "    #history.history['val_loss']\n",
    "    \n",
    "    return triplet_model\n",
    "\n",
    "\n",
    "def triplet_acc(imageTrips):\n",
    "    '''\n",
    "    Run the model and let model conduct prediction \n",
    "    \n",
    "    Argument:\n",
    "        imageTrips: input image\n",
    "        \n",
    "    Return:\n",
    "        result: prediction results\n",
    "    '''\n",
    "    imageTrips = imageTrips.reshape(len(imageTrips), 3, 105, 105, 3)\n",
    "    result = []\n",
    "    predictionP = triplet_model.layers[3].predict(imageTrips[:,1])\n",
    "    predictionN = triplet_model.layers[3].predict(imageTrips[:,2])\n",
    "    \n",
    "    for i in range(len(imageTrips)):\n",
    "        posDist = sum(predictionP[i])\n",
    "        negDist = sum(predictionN[i])\n",
    "        if posDist > negDist:\n",
    "            result.append(0)\n",
    "        if posDist < negDist:\n",
    "            result.append(1)\n",
    "    return result\n",
    "\n",
    "def Results(tripletLitst, testList, testlabel, label):\n",
    "    '''\n",
    "    Arguments: \n",
    "        tripletLitsts -- triplet data list\n",
    "        testList -- test data list\n",
    "        testlabel -- test label list\n",
    "        label -- training label list\n",
    "       \n",
    "    Return:\n",
    "        results -- accuaracy when performed on tripletLitst\n",
    "        resultsTest -- accuaracy when performed on testList\n",
    "        resultsMix -- accuracy when performed on the mixt of trpletLitst abd testList\n",
    "        mixX -- a combined NumPy array of testList (from the 1st to the 1000th elements) and triplet litst \n",
    "        (from the 1st to the 1000th elements).        \n",
    "        mixY -- a combined NumPy array of testLable (from the 1st to the 1000th elements) and label\n",
    "        (from the 1st to the 1000th elements).\n",
    "    '''\n",
    "    results = triplet_acc(tripletLitst)\n",
    "    resultsTest = triplet_acc(testList)\n",
    "    \n",
    "    mixX = np.concatenate((testList[:1000], tripletLitst[:1000]))\n",
    "    mixY = np.concatenate((testlabel[:1000], label[:1000]))\n",
    "    \n",
    "    resultsMix = triplet_acc(mixX)\n",
    "    \n",
    "    return results, resultsTest, resultsMix, mixX, mixY\n",
    "\n",
    "\n",
    "def get_acc(predY, trueY):\n",
    "    '''\n",
    "    Calculate accuracy of CNN with triplet loss\n",
    "    \n",
    "    Arguments:\n",
    "        predY -- predicted value\n",
    "        trueY -- the true value\n",
    "        \n",
    "    Return:\n",
    "        accuracy -- the accuracy of the model with triplet loss function\n",
    "    '''\n",
    "    count = 0\n",
    "    for i in range(len(predY)):\n",
    "        if predY[i] == trueY[i]:\n",
    "            count = count+1\n",
    "        else:\n",
    "            pass\n",
    "    accuracy = (count/len(predY))*100\n",
    "    return accuracy \n",
    "\n",
    "\n",
    "def print_acc(results, label, resultsTest, testlabel, resultsMix, mixY):\n",
    "    '''\n",
    "    Print out the accuracy of triplet loss CNN for each types of results\n",
    "    \n",
    "    Arguments:\n",
    "        results -- accuaracy when performed on tripletLitst\n",
    "        label -- training label list.\n",
    "        resultsTest -- accuaracy when performed on testList\n",
    "        testlabel -- test label list\n",
    "        resultsMix -- accuracy when performed on the mixt of trpletLitst abd testList\n",
    "        mixY -- a combined NumPy array of testLable (from the 1st to the 1000th elements) and label\n",
    "        (from the 1st to the 1000th elements).\n",
    "        \n",
    "    Print:\n",
    "        \"Accuracy for triplet loss CNN on {dataTypes} is {dataPrediction}%\". It prints out the accuracy \n",
    "        for triplet loss CNN on each data type\n",
    "        \n",
    "    '''\n",
    "    train_prediction_acc = get_acc(results, label)\n",
    "    print(\"Accuracy for triplet loss CNN on training data is {}%\".format(train_prediction_acc))\n",
    "    test_prediction_acc = get_acc(resultsTest, testlabel)\n",
    "    print(\"Accuracy for triplet loss CNN on test data is {}%\".format(test_prediction_acc))\n",
    "    #mix_prediction_acc = get_acc(resultsMix, mixY)\n",
    "    #print(\"Accuracy for triplet loss CNN on mixed data of trainig and test data is {}%\".format(mix_prediction_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8faeec48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9993304, 'Brendan', 'Wallace-Nash'), (9300449, 'Min-Pu', 'Tsai'), (10661450, 'Bingqing', 'Qian')]\n",
      "Triplet loss computed with numpy 0.2633540523052216\n",
      "Metal device set to: Apple M1 Pro\n",
      "Triplet loss computed with tensorflow 0.26335403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:06:48.290988: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-11 10:06:48.291081: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-05-11 10:06:48.292689: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-05-11 10:06:48.292735: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-11 10:06:48.303460: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-11 10:06:48.303477: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss epochs processing. Now building model...\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:07:04.477342: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-11 10:07:04.477365: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-05-11 10:07:04.488475: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-11 10:07:04.508721: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-11 10:07:04.603680: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - ETA: 0s - loss: 0.2864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2022-05-11 10:07:55.003962: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 56s 3ms/sample - loss: 0.2864 - val_loss: 0.0280\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 54s 3ms/sample - loss: 0.0787 - val_loss: 0.0295\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 54s 3ms/sample - loss: 0.0459 - val_loss: 0.0295\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 54s 3ms/sample - loss: 0.0459 - val_loss: 0.0295\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 54s 3ms/sample - loss: 0.0459 - val_loss: 0.0295\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 54s 3ms/sample - loss: 0.0459 - val_loss: 0.0295\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 54s 3ms/sample - loss: 0.0459 - val_loss: 0.0295\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 55s 3ms/sample - loss: 0.0459 - val_loss: 0.0295\n",
      "Contrative Loss epochs processing. Now building model...\n",
      "WARNING:tensorflow:From /Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22400 samples, validate on 5600 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:14:33.686757: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-11 10:14:33.742816: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-11 10:14:33.802765: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-05-11 10:14:33.846094: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22400/22400 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.5274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:14:51.471064: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22400/22400 [==============================] - 20s 887us/sample - loss: 0.2923 - accuracy: 0.5274 - val_loss: 0.2867 - val_accuracy: 0.5098\n",
      "Epoch 2/10\n",
      "22400/22400 [==============================] - 14s 640us/sample - loss: 0.2274 - accuracy: 0.6505 - val_loss: 0.2126 - val_accuracy: 0.7130\n",
      "Epoch 3/10\n",
      "22400/22400 [==============================] - 14s 621us/sample - loss: 0.2059 - accuracy: 0.7215 - val_loss: 0.2015 - val_accuracy: 0.7296\n",
      "Epoch 4/10\n",
      "22400/22400 [==============================] - 14s 636us/sample - loss: 0.2005 - accuracy: 0.7245 - val_loss: 0.1987 - val_accuracy: 0.7296\n",
      "Epoch 5/10\n",
      "22400/22400 [==============================] - 14s 641us/sample - loss: 0.1994 - accuracy: 0.7245 - val_loss: 0.1984 - val_accuracy: 0.7296\n",
      "Epoch 6/10\n",
      "22400/22400 [==============================] - 15s 667us/sample - loss: 0.1990 - accuracy: 0.7245 - val_loss: 0.1982 - val_accuracy: 0.7296\n",
      "Epoch 7/10\n",
      "22400/22400 [==============================] - 14s 642us/sample - loss: 0.1991 - accuracy: 0.7245 - val_loss: 0.1978 - val_accuracy: 0.7296\n",
      "Epoch 8/10\n",
      "22400/22400 [==============================] - 14s 630us/sample - loss: 0.1990 - accuracy: 0.7245 - val_loss: 0.1979 - val_accuracy: 0.7296\n",
      "Epoch 9/10\n",
      "22400/22400 [==============================] - 14s 635us/sample - loss: 0.1990 - accuracy: 0.7245 - val_loss: 0.2001 - val_accuracy: 0.7296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendanwallace-nash/tensorflow-test/env/lib/python3.9/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2022-05-11 10:16:48.228820: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is 72.55%\n",
      "test accuracy is 73.14285714285714%\n",
      "mix of train and test accuracy is 74.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 10:19:41.558190: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for triplet loss CNN on training data is 82.78999999999999%\n",
      "Accuracy for triplet loss CNN on test data is 67.675%\n",
      "Accuracy for triplet loss CNN on mixed data of trainig and test data is 74.2%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(my_team())\n",
    "    #Below should print Triplet loss computed predictions\n",
    "    triplet_loss_test()\n",
    "    \n",
    "    #Below should assign data_prepare()\n",
    "    arrayImage, arrayAlph, x_test, y_test, x_train, y_train = data_prepare()\n",
    "    \n",
    "    #Below should assign global_variable1 &2\n",
    "    label, tripletLitst = global_variable1(arrayImage, arrayAlph)\n",
    "    testlabel, testList = global_variable2(arrayImage, arrayAlph)\n",
    "    \n",
    "    #Below should assign dataProcess()\n",
    "    arrayImage, arrayAlph, x_test, y_test, x_train, y_train = dataProcess(arrayImage, arrayAlph, \n",
    "                                                                          x_test, y_test, \n",
    "                                                                          tripletLitst)\n",
    "    \n",
    "    print(\"Triplet Loss epochs processing. Now building model...\")\n",
    "    triplet_model = triplet_embedding(tripletLitst, label,testList, x_test, y_test, x_train, y_train)\n",
    "    print(\"Contrative Loss epochs processing. Now building model...\")\n",
    "    siamese_model = contrastive_loss_patch()\n",
    "    \n",
    "    #Below should assign Results()\"\n",
    "    results, resultsTest, resultsMix, mixX, mixY = Results(tripletLitst, testList, testlabel, label)    \n",
    "    #Below should print out 3 Accuracy for triplet loss CNN\n",
    "    print_acc(results, label, resultsTest, testlabel, resultsMix, mixY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa0abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4fc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
